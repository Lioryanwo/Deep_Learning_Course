{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f64530-72bb-46ee-9996-456655f2f82b",
   "metadata": {},
   "source": [
    "Libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0878ebbc-ff83-45ba-b031-2ee8cb7baa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19698c9f-2c22-4f93-b1c8-540117fba772",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# stage 1 Dataset Selection:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mliory\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mStock Market Dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstocks:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(base_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mstocks\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124metfs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(base_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124metfs\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# stage 1 Dataset Selection:\n",
    "base_dir = r\"C:\\Users\\liory\\Downloads\\Stock Market Dataset\"\n",
    "\n",
    "print(\"stocks:\", len(os.listdir(base_dir + r\"\\stocks\")))\n",
    "print(\"etfs:\", len(os.listdir(base_dir + r\"\\etfs\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58adba-ee9a-4fa2-8195-f8e23cc510e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_dir = r\"C:\\Users\\liory\\Downloads\\Stock Market Dataset\\stocks\"\n",
    "etf_dir    = r\"C:\\Users\\liory\\Downloads\\Stock Market Dataset\\etfs\"\n",
    "\n",
    "stock_files = glob.glob(stocks_dir + r\"\\*.csv\")\n",
    "etf_files   = glob.glob(etf_dir   + r\"\\*.csv\")\n",
    "\n",
    "print(\"number of stock files:\", len(stock_files))\n",
    "print(\"number of ETF files:\", len(etf_files))\n",
    "\n",
    "df = pd.read_csv(stock_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa6a186-6db1-4287-bfec-2e3c4ad0f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "df.info()\n",
    "df.describe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508859f-4e1d-47f5-bfa2-7e0336139a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(df[\"Date\"], df[\"Close\"])\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close\")\n",
    "plt.title(\"Close Price Over Time\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aab096-f0fb-40a5-a03a-d9a6bba89093",
   "metadata": {},
   "outputs": [],
   "source": [
    "## stage 2 Build Model:\n",
    "values = df[[\"Close\"]].values.astype(\"float32\")\n",
    "\n",
    "n = len(values)\n",
    "train_end = int(n * 0.70)\n",
    "val_end   = int(n * 0.85)\n",
    "\n",
    "train_vals = values[:train_end]\n",
    "val_vals   = values[train_end:val_end]\n",
    "test_vals  = values[val_end:]\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_scaled = scaler.fit_transform(train_vals)\n",
    "val_scaled   = scaler.transform(val_vals)\n",
    "test_scaled  = scaler.transform(test_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc684b3-5357-4b29-a19e-79dabc5f939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_windows(data, seq_len=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_len):\n",
    "        X.append(data[i:i+seq_len])\n",
    "        y.append(data[i+seq_len])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "SEQ_LEN = 30\n",
    "\n",
    "X_train, y_train = make_windows(train_scaled, SEQ_LEN)\n",
    "X_val,   y_val   = make_windows(val_scaled, SEQ_LEN)\n",
    "X_test,  y_test  = make_windows(test_scaled, SEQ_LEN)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c68d1-0358-4646-b990-b4735b10e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)  \n",
    "        self.y = torch.tensor(y, dtype=torch.float32)  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader   = DataLoader(TimeSeriesDataset(X_val, y_val), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(TimeSeriesDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# בדיקה\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"X batch:\", xb.shape, \"y batch:\", yb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b2138b-1c0a-495e-bcfa-c35ba6601933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)     \n",
    "        last = out[:, -1, :]    \n",
    "        y = self.fc(last)          \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68964bdc-866f-402c-9010-309d72d80ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRURegressor(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.fc(last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c53c2c-d84c-4eb8-ab7d-9f41fbe1cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage 3 model train and test\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LSTMRegressor(input_size=1, hidden_size=64, num_layers=2, dropout=0.2).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def run_epoch(model, loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        preds = model(Xb)                \n",
    "        loss = criterion(preds, yb)      \n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": []}\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = run_epoch(model, train_loader, train=True)\n",
    "    val_loss   = run_epoch(model, val_loader, train=False)\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} | train_loss={train_loss:.6f} | val_loss={val_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b00cd8-c0e5-4127-b149-51be2a08ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history[\"train_loss\"], label=\"Train\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a0c58-addf-4a56-9797-b74537aa35aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "preds_list = []\n",
    "y_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in test_loader:\n",
    "        Xb = Xb.to(device)\n",
    "        preds = model(Xb).cpu().numpy()  \n",
    "        preds_list.append(preds)\n",
    "        y_list.append(yb.numpy())        \n",
    "\n",
    "preds_scaled = np.vstack(preds_list)  \n",
    "y_scaled     = np.vstack(y_list)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9617e9-806d-41a7-8bd4-b0df595a9aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = scaler.inverse_transform(preds_scaled)\n",
    "y_true = scaler.inverse_transform(y_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e27f6e-1215-4a78-b95f-15a50d08fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean((preds - y_true) ** 2))\n",
    "print(\"Test RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb6534e-81be-41de-9611-467a3e5d97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(y_true[:200], label=\"Actual\")\n",
    "plt.plot(preds[:200], label=\"Predicted\")\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.title(\"Actual vs Predicted (first 200 test points)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e8f42f-aa4c-40ed-b157-2b9382c3253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51eeeca-5508-44b8-bb8d-3311cc4fcc71",
   "metadata": {},
   "source": [
    "Preprocessing:\n",
    "I converted the Date column into a datetime format and sorted the dataset chronologically. Since this is a time-series task, I performed a time-based split into 70% training, 15% validation, and 15% testing, without shuffling to preserve temporal order.\n",
    "For scaling, I used MinMaxScaler, fitting it only on the training set to prevent future data leakage, then transforming the validation and test sets using the same scaler.\n",
    "After scaling, I generated sliding windows with seq_len=30, where each sample consists of 30 past time steps and the label represents the next value in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49164899-eeb5-4fd8-9cda-689826c8554b",
   "metadata": {},
   "source": [
    "Architecture:\n",
    "I implemented an LSTM-based regressor with hidden_size=64, num_layers=2, and dropout=0.2 for regularization.\n",
    "The final layer is a fully connected Linear unit that outputs a single predicted value for each input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cefd0a-2560-4aa7-a7f6-134660bd2610",
   "metadata": {},
   "source": [
    "Training & Validation:\n",
    "The model was trained using MSELoss as the objective function and optimized with the Adam optimizer at a learning rate of lr=1e-3.\n",
    "Training was performed with batch_size=64 for 20 epochs, evaluating performance on the validation set after each epoch to track loss improvement and avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df532dd8-2f10-4e62-b8c4-c301c99e597a",
   "metadata": {},
   "source": [
    "Results & Evaluation:\n",
    "I logged training and validation loss values and visualized them using a train/val loss plot.\n",
    "Final performance was evaluated on the test set using RMSE, and I also plotted Actual vs Predicted values over the first 200 test points to verify that the model follows the historical trend and produces reasonable future forecasts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
